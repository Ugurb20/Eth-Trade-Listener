@startuml EthereumTransactionListener

skinparam classAttributeIconSize 0
skinparam linetype ortho
skinparam packageStyle rectangle

package "External Libraries" <<Cloud>> {
    class WebSocketProvider <<ethers>> {
        +websocket
        +ready
        +getNetwork()
        +getTransaction(hash)
        +on(event, listener)
        +off(event)
        +destroy()
    }

    class Kafka <<kafkajs>> {
        +producer()
    }

    class Producer <<kafkajs>> {
        +connect()
        +disconnect()
        +send(record)
    }

    class SparkSession <<pyspark>> {
        +readStream
        +writeStream
        +stop()
        +sparkContext
    }

    class KafkaSource <<spark-kafka>> {
        +format(kafka)
        +option(key, value)
        +load()
    }

    class PostgreSQL <<database>> {
        +JDBC Driver
        +psycopg2 Driver
    }

    class TheGraphAPI <<API>> {
        +query(graphql)
        +fetch pool data
    }

    class FourByteAPI <<API>> {
        +search signatures
        +get function data
    }
}

' Configuration Types
package "Configuration Types" {
    interface EthereumListenerConfig {
        +maxConcurrentFetches: number
        +fetchTimeout: number
        +autoReconnect: boolean
        +maxReconnectAttempts: number
        +reconnectDelay: number
        +maxReconnectDelay: number
        +reconnectBackoffMultiplier: number
    }

    interface KafkaProducerConfig {
        +brokers: string[]
        +clientId: string
        +topic: string
        +compression: boolean
        +maxRetries: number
        +retryTimeout: number
    }

    interface SparkConsumerConfig {
        +kafkaBootstrapServers: string
        +kafkaTopic: string
        +consumerGroupId: string
        +startingOffsets: string
        +autoCommitEnabled: boolean
        +autoCommitIntervalMs: number
    }

    interface EnvironmentConfig {
        +ETH_WEBSOCKET_URL: string
        +MAX_CONCURRENT_FETCHES: string
        +FETCH_TIMEOUT: string
        +AUTO_RECONNECT: string
        +MAX_RECONNECT_ATTEMPTS: string
        +RECONNECT_DELAY: string
        +MAX_RECONNECT_DELAY: string
        +RECONNECT_BACKOFF_MULTIPLIER: string
        +KAFKA_BOOTSTRAP_SERVERS: string
        +CONSUMER_GROUP_ID: string
    }
}

' Data Types
package "Data Types" {
    interface NormalizedTransaction {
        +hash: string
        +blockNumber: number
        +from: string
        +to: string
        +value: string
        +gasLimit: string
        +gasPrice: string
        +maxFeePerGas: string
        +maxPriorityFeePerGas: string
        +data: string
        +nonce: number
        +type: number
        +chainId: string
        +metadata: TransactionMetadata
    }

    interface TransactionMetadata {
        +receivedAt: string
        +network: string
        +chainId: string
    }

    interface TransactionFetchResult {
        +success: boolean
        +txHash: string
        +transaction: TransactionInfo
        +error: string
    }

    interface KafkaMessageMetadata {
        +key: string
        +value: string
        +timestamp: number
    }

}

package "Core Components" {
    class EthereumWebSocketListener {
        -websocketUrl: string
        -provider: WebSocketProvider
        -isConnected: boolean
        -isListening: boolean
        -maxConcurrentFetches: number
        -fetchTimeout: number
        -ongoingFetches: Set
        -networkName: string
        -chainId: string
        -transactionCallback: TransactionCallback
        -autoReconnect: boolean
        -maxReconnectAttempts: number
        -reconnectDelay: number
        -maxReconnectDelay: number
        -reconnectBackoffMultiplier: number
        -reconnectAttempts: number
        -isReconnecting: boolean
        -reconnectTimer: Timer
        -shouldReconnect: boolean
        -wasListeningBeforeDisconnect: boolean
        -connectionId: number
        __
        +EthereumWebSocketListener(websocketUrl, config)
        +connect()
        +disconnect()
        +startListening()
        +stopListening()
        +setTransactionCallback(callback)
        +getConnectionStatus()
        +getListeningStatus()
        +getConnectionId()
        +getReconnectionState()
        -setupErrorHandlers(connectionId)
        -cleanupProvider()
        -scheduleReconnect()
        -reconnect()
        -clearReconnectTimer()
        -handlePendingTransaction(txHash)
        -fetchTransactionDetails(txHash)
    }

    class TransactionKafkaProducer {
        -kafka: Kafka
        -producer: Producer
        -topic: string
        -isConnected: boolean
        -maxRetries: number
        -retryTimeout: number
        __
        +TransactionKafkaProducer(config)
        +connect()
        +disconnect()
        +publishTransaction(transaction)
        +publishBatch(transactions)
        +getConnectionStatus()
        +getTopic()
    }

    class SparkKafkaConsumer <<python>> {
        -spark: SparkSession
        -kafkaBootstrapServers: string
        -topic: string
        -consumerGroupId: string
        -transactionSchema: StructType
        -postgresUrl: string
        -postgresProperties: dict
        __
        +SparkKafkaConsumer()
        +createSparkSession()
        +getTransactionSchema()
        +getPostgresUrl()
        +getPostgresProperties()
        +consumeKafkaMessages()
        +writeToPostgres(batch_df, batch_id)
        -parseMessages()
        -upsertPartition(partition)
    }

    class DimensionTableScraper <<typescript>> {
        -db: DatabaseManager
        -fourByteScraper: FourByteScraper
        -uniswapGraphScraper: UniswapGraphScraper
        __
        +DimensionTableScraper(dbConfig, graphApiKey)
        +run()
        -scrapeContracts()
        -scrapeFunctions()
        -deduplicateContracts()
        -deduplicateFunctions()
    }

    class FourByteScraper <<typescript>> {
        __
        +getSwapFunctions()
        +getLiquidityFunctions()
        -fetchFromAPI(query)
    }

    class UniswapGraphScraper <<typescript>> {
        -apiKey: string
        __
        +UniswapGraphScraper(apiKey)
        +fetchAllContracts(limit)
        -querySubgraph(graphql)
    }

    class DatabaseManager <<typescript>> {
        -pool: PostgresPool
        __
        +DatabaseManager(config)
        +connect()
        +close()
        +upsertContracts(contracts)
        +upsertFunctions(functions)
    }

    class PostgreSQLDatabase <<persistence>> {
        +ethereum_transactions_raw: Table
        +dim_contract: Table
        +dim_function: Table
        +dim_calldata_slice: Table
        __
        +storeTransaction()
        +queryTransactions()
        +storeDimData()
    }
}

package "Utilities" {
    class TransactionNormalizer <<utility>> {
        +{static} normalizeTransaction(transaction, networkName, chainId)
        +{static} transactionToJSON(normalized)
        +{static} transactionToCompactJSON(normalized)
    }
}

package "Type Aliases" {
    interface TransactionCallback <<function>> {
        +call(transaction)
    }

    interface ReconnectionState {
        +isReconnecting: boolean
        +reconnectAttempts: number
        +shouldReconnect: boolean
    }
}

class Main <<entrypoint>> {
    +{static} main()
}

class SparkConsumerMain <<python-entrypoint>> {
    +{static} main()
}

class ScraperMain <<typescript-entrypoint>> {
    +{static} main()
}

' Relationships - Listener Service
EthereumWebSocketListener --> WebSocketProvider : uses
EthereumWebSocketListener ..> EthereumListenerConfig : configured by
EthereumWebSocketListener ..> TransactionCallback : uses
EthereumWebSocketListener ..> NormalizedTransaction : produces
EthereumWebSocketListener ..> TransactionNormalizer : calls

TransactionKafkaProducer --> Kafka : creates
TransactionKafkaProducer --> Producer : uses
TransactionKafkaProducer ..> KafkaProducerConfig : configured by
TransactionKafkaProducer ..> NormalizedTransaction : consumes
TransactionKafkaProducer ..> TransactionNormalizer : calls

' Relationships - Spark Consumer
SparkKafkaConsumer --> SparkSession : creates
SparkKafkaConsumer --> KafkaSource : uses
SparkKafkaConsumer --> PostgreSQL : writes to
SparkKafkaConsumer --> PostgreSQLDatabase : persists
SparkKafkaConsumer ..> SparkConsumerConfig : configured by
SparkKafkaConsumer ..> NormalizedTransaction : consumes

' Relationships - Dimension Scraper
DimensionTableScraper --> DatabaseManager : uses
DimensionTableScraper --> FourByteScraper : uses
DimensionTableScraper --> UniswapGraphScraper : uses
FourByteScraper --> FourByteAPI : queries
UniswapGraphScraper --> TheGraphAPI : queries
DatabaseManager --> PostgreSQLDatabase : writes to

' Utility Relationships
TransactionNormalizer ..> NormalizedTransaction : creates
TransactionNormalizer ..> TransactionMetadata : creates

NormalizedTransaction *-- TransactionMetadata : contains

' Entry Points
Main --> EthereumWebSocketListener : creates
Main --> TransactionKafkaProducer : creates
Main ..> TransactionCallback : sets

SparkConsumerMain --> SparkKafkaConsumer : creates

ScraperMain --> DimensionTableScraper : creates

' Notes
note right of EthereumWebSocketListener
  Manages WebSocket connection to Ethereum RPC
  - Auto-reconnection with exponential backoff
  - Concurrency control for transaction fetches
  - Connection ID tracking for ghost session prevention
end note

note right of TransactionKafkaProducer
  Publishes normalized transactions to Kafka
  - GZIP compression
  - Idempotent producer
  - Batch publishing support
end note

note right of SparkKafkaConsumer
  Consumes transactions from Kafka using PySpark
  - Spark Structured Streaming with Kafka integration
  - Auto-commit enabled (1s interval)
  - Schema validation and type casting
  - Batch processing with foreachBatch
  - PostgreSQL persistence with upsert logic
  - Duplicate handling (ON CONFLICT DO NOTHING)
end note

note right of PostgreSQLDatabase
  Persistent storage for Ethereum transactions and metadata

  <b>Transaction Table:</b>
  - ethereum_transactions_raw
  - Indexed on: hash, block_number, addresses, timestamps
  - PRIMARY KEY: hash (deduplication)

  <b>Dimension Tables:</b>
  - dim_contract (DeFi protocol contracts)
  - dim_function (function signatures)
  - dim_calldata_slice (calldata parsing rules)

  All tables use ON CONFLICT for upsert logic
end note

note right of DimensionTableScraper
  Populates dimension tables for transaction decoding

  <b>Data Sources:</b>
  - The Graph API: Uniswap pool contracts
  - 4byte.directory: Function signatures

  <b>Workflow:</b>
  1. Scrape contract addresses from subgraphs
  2. Scrape function signatures from 4byte
  3. Deduplicate and upsert into PostgreSQL

  Runs once on startup (restart: "no")
end note

note bottom of TransactionNormalizer
  Converts raw transactions to normalized format
  - Adds metadata (timestamp, network, chainId)
  - Handles both legacy and EIP-1559 transactions
end note

' Architecture Flow
note as N1
  <b>Complete End-to-End Data Flow</b>

  <b>1. Dimension Table Population (TypeScript - Run Once)</b>
  • DimensionTableScraper runs on startup
  • Fetches pool contracts from The Graph API (Uniswap subgraph)
  • Fetches function signatures from 4byte.directory
  • Populates dim_contract and dim_function tables
  • Exits after completion (restart: "no")

  <b>2. Transaction Capture (TypeScript - Continuous)</b>
  • EthereumWebSocketListener connects to Ethereum WebSocket RPC
  • Subscribes to pending transactions from mempool
  • Fetches full transaction details with concurrency control
  • Auto-reconnection with exponential backoff on disconnect

  <b>3. Normalization & Publishing (TypeScript)</b>
  • TransactionNormalizer extracts key fields and adds metadata
  • TransactionKafkaProducer publishes to Kafka topic
  • GZIP compression, idempotent producer

  <b>4. Stream Processing (PySpark - Continuous)</b>
  • SparkKafkaConsumer reads from Kafka using Structured Streaming
  • Parses and validates transaction schema
  • Type casting for numeric fields (NUMERIC(38,0), BIGINT)
  • Batch processing via foreachBatch

  <b>5. Persistence (PostgreSQL)</b>
  • Writes to ethereum_transactions_raw via JDBC and psycopg2
  • Upsert logic: INSERT ... ON CONFLICT (hash) DO NOTHING
  • Indexed for efficient queries by address, block, network
  • Auto-commits Kafka offsets after successful write
  • Dimension tables available for transaction decoding/enrichment
end note

@enduml
