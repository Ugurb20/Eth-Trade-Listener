ðŸ”¹ Step 1 â€” Ethereum WebSocket Listener (Conceptual Overview)
1. Purpose

Create a lightweight service that connects to an Ethereum node via WebSocket, receives real-time pending transactions, filters them, and sends the relevant ones into Kafka for downstream processing.

This component is the â€œentry pointâ€ of the data pipeline.

2. Requirements (High-Level)
a. WebSocket-enabled Ethereum RPC endpoint

Must support WebSocket (wss://â€¦)

Can be from Infura, Alchemy, QuickNode, or your own node

Needed to stream pending and/or confirmed transactions

b. Lightweight Node.js service

Establishes the WebSocket connection

Subscribes to transaction events

Deserializes incoming data and transforms it into a structured JSON payload

c. Kafka cluster

Local (Docker)

One topic, e.g.: blockchain.txs.raw

The listener acts as a producer, publishing messages to Kafka

d. Basic filters (optional but recommended)

Examples:

Only transactions with value > 0

Only transactions to known contracts

Only certain wallets

Sampling to reduce volume

This prevents unnecessary load on Kafka and Spark.

3. Main Flow (Step-by-Step Logic)
1. Connect to WebSocket

The listener opens a persistent WebSocket connection to the Ethereum RPC provider.

2. Listen for pending transaction hashes

Ethereum nodes broadcast pending transaction hashes as they enter the mempool.

3. Fetch full transaction objects

For each hash:

Request full transaction details

Skip if the node does not return data (common when overloaded)

4. Normalize into a JSON payload

Extract only the important fields:

hash, blockNumber, from, to, value, gas, data, timestamp, chain, etc.

Add your own metadata, such as received_at

5. Push into Kafka

Each transaction becomes a Kafka message:

Key: transaction hash

Value: JSON payload

6. Handle WebSocket disconnects

Implement automatic reconnection (WebSocket providers often disconnect after idle periods).

7. Handle rate limits

If the node rate-limits:

Slow down consumption

Add retry logic

Optionally batch RPC calls

4. Output

At the end of Step 1, you have:

âœ” A real-time stream of Ethereum transactions

coming through WebSocket

âœ” Structured transaction data pushed into Kafka

In a single topic (blockchain.txs.raw)

âœ” A clean entry point for Spark Structured Streaming

which will later consume this Kafka topic for ETL, cleaning, enrichment, and storage.

5. What This Step Achieves

Real-time ingestion from the blockchain

Reliable buffering via Kafka

A scalable architecture for downstream analytics

Foundation for Spark streaming jobs, dashboards, and aggregations

